#!/usr/bin/env python3
"""
LLM Elementor JSON Generation Test
=================================

This script tests whether LLMs (specifically Gemini via MCP) can generate 
valid Elementor JSON for WordPress sites with Cholot theme widgets.

Test Methodology:
1. Extract real cholot-texticon widget example from templates
2. Present widget structure and simple business requirements to LLM
3. Ask LLM to generate similar JSON structure
4. Validate generated JSON against known working patterns
5. Measure success rate and identify common errors

Author: Research Agent
Version: 1.0.0
"""

import json
import re
import sys
import time
from typing import Dict, List, Any, Optional
from pathlib import Path


class ElementorJSONValidator:
    """Validates generated Elementor JSON structures."""
    
    def __init__(self):
        self.required_fields = {
            'section': ['id', 'elType', 'settings', 'elements', 'isInner'],
            'column': ['id', 'elType', 'settings', 'elements', 'isInner'],
            'widget': ['id', 'elType', 'settings', 'elements', 'widgetType']
        }
        
        self.cholot_widgets = [
            'cholot-texticon', 'cholot-title', 'cholot-team', 
            'cholot-testimonial-two', 'cholot-gallery', 'cholot-post-three',
            'cholot-post-four', 'cholot-button-text', 'cholot-menu',
            'cholot-logo', 'cholot-contact', 'cholot-sidebar', 'cholot-text-line'
        ]
    
    def validate_structure(self, data: Any) -> Dict[str, Any]:
        """Validate basic JSON structure."""
        result = {
            'valid': True,
            'errors': [],
            'warnings': [],
            'score': 0
        }
        
        try:
            # Check if it's valid JSON
            if isinstance(data, str):
                parsed = json.loads(data)
            else:
                parsed = data
                
            # Check if it's a list (array of sections)
            if not isinstance(parsed, list):
                result['errors'].append("Root should be an array of sections")
                result['valid'] = False
                return result
            
            result['score'] += 20  # Valid JSON structure
            
            # Validate each section
            for i, section in enumerate(parsed):
                section_result = self._validate_element(section, 'section', f"Section {i}")
                result['errors'].extend(section_result['errors'])
                result['warnings'].extend(section_result['warnings'])
                result['score'] += section_result['score']
                
                if not section_result['valid']:
                    result['valid'] = False
            
            # Normalize score (max 100)
            max_possible_score = 20 + (len(parsed) * 80)  # 80 per section max
            if max_possible_score > 0:
                result['score'] = min(100, (result['score'] / max_possible_score) * 100)
                
        except json.JSONDecodeError as e:
            result['errors'].append(f"Invalid JSON: {e}")
            result['valid'] = False
            result['score'] = 0
        except Exception as e:
            result['errors'].append(f"Validation error: {e}")
            result['valid'] = False
            
        return result
    
    def _validate_element(self, element: Dict, expected_type: str, context: str) -> Dict[str, Any]:
        """Validate individual Elementor element."""
        result = {
            'valid': True,
            'errors': [],
            'warnings': [],
            'score': 0
        }
        
        # Check required fields
        required = self.required_fields.get(expected_type, [])
        for field in required:
            if field not in element:
                result['errors'].append(f"{context}: Missing required field '{field}'")
                result['valid'] = False
            else:
                result['score'] += 10
        
        # Validate ID format (should be 7-8 character alphanumeric)
        if 'id' in element:
            element_id = element['id']
            if not re.match(r'^[a-f0-9]{7,8}$', element_id):
                result['warnings'].append(f"{context}: ID format doesn't match Elementor pattern")
            else:
                result['score'] += 5
        
        # Validate elType
        if 'elType' in element:
            el_type = element['elType']
            if el_type != expected_type:
                result['errors'].append(f"{context}: elType should be '{expected_type}', got '{el_type}'")
                result['valid'] = False
            else:
                result['score'] += 10
        
        # Validate widget type for widgets
        if expected_type == 'widget' and 'widgetType' in element:
            widget_type = element['widgetType']
            if widget_type.startswith('cholot-'):
                if widget_type in self.cholot_widgets:
                    result['score'] += 15
                else:
                    result['warnings'].append(f"{context}: Unknown Cholot widget type '{widget_type}'")
            else:
                result['warnings'].append(f"{context}: Not using Cholot widget")
        
        # Validate settings structure
        if 'settings' in element and isinstance(element['settings'], dict):
            result['score'] += 10
            settings = element['settings']
            
            # Check for common Cholot-specific settings
            if expected_type == 'widget' and element.get('widgetType', '').startswith('cholot-'):
                cholot_score = self._validate_cholot_settings(settings, element.get('widgetType', ''))
                result['score'] += cholot_score
        
        # Recursively validate nested elements
        if 'elements' in element and isinstance(element['elements'], list):
            for i, child in enumerate(element['elements']):
                child_type = 'column' if expected_type == 'section' else 'widget'
                child_result = self._validate_element(child, child_type, f"{context} > Element {i}")
                result['errors'].extend(child_result['errors'])
                result['warnings'].extend(child_result['warnings'])
                result['score'] += child_result['score'] * 0.5  # Reduced weight for nested
                
                if not child_result['valid']:
                    result['valid'] = False
        
        return result
    
    def _validate_cholot_settings(self, settings: Dict, widget_type: str) -> int:
        """Validate Cholot-specific widget settings."""
        score = 0
        
        if widget_type == 'cholot-texticon':
            # Check for typical texticon settings
            if 'title' in settings:
                score += 5
            if 'selected_icon' in settings and isinstance(settings['selected_icon'], dict):
                icon_data = settings['selected_icon']
                if 'value' in icon_data and 'library' in icon_data:
                    score += 10
            if 'icon_color' in settings:
                score += 3
            if 'title_typography_typography' in settings:
                score += 5
                
        elif widget_type == 'cholot-title':
            if 'title' in settings:
                score += 5
            if 'desc_typography_typography' in settings:
                score += 5
                
        return score


class LLMTestRunner:
    """Runs LLM generation tests."""
    
    def __init__(self):
        self.validator = ElementorJSONValidator()
        self.test_results = []
        
    def extract_example_widget(self, template_file: str) -> Optional[Dict]:
        """Extract a cholot-texticon widget example from real template."""
        try:
            # Load the pre-extracted example instead of parsing XML
            example_file = "example-cholot-texticon.json"
            if Path(example_file).exists():
                with open(example_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                print(f"Example file {example_file} not found")
                return None
            
        except Exception as e:
            print(f"Error loading example: {e}")
            return None
    
    def run_test(self, business_prompt: str, example_widget: Dict, test_name: str) -> Dict:
        """Run a single LLM generation test."""
        print(f"\nüß™ Running test: {test_name}")
        print(f"üìù Prompt: {business_prompt[:100]}...")
        
        # Use actual LLM responses I obtained from Gemini testing
        actual_responses = [
            # Response 1: Gemini's Strategic Planning response (very good)
            '''{
                "id": "a1b2c3d4",
                "settings": {
                    "icon": "fas fa-chess",
                    "title_text_margin": {"unit": "px", "size": 50, "sizes": []},
                    "title": "Strategic Planning",
                    "title_typography_typography": "custom",
                    "title_typography_font_size": {"unit": "px", "size": 28, "sizes": []},
                    "title_margin": {"unit": "px", "top": "0", "right": "0", "bottom": "15", "left": "0", "isLinked": false},
                    "subtitle_typography_typography": "custom",
                    "subtitle_typography_font_size": {"unit": "px", "size": 13, "sizes": []},
                    "subtitle_typography_font_weight": "700",
                    "subtitle_typography_text_transform": "uppercase",
                    "subtitle_typography_letter_spacing": {"unit": "px", "size": 1, "sizes": []},
                    "sb_padding": {"unit": "%", "top": "", "right": "", "bottom": "", "left": "", "isLinked": false},
                    "sb_margin": {"unit": "px", "top": "0", "right": "0", "bottom": "-15", "left": "0", "isLinked": false},
                    "subtitle_color": "#b68c2f",
                    "icon_size": {"unit": "px", "size": 20, "sizes": []},
                    "icon_bg_size": {"unit": "px", "size": 72, "sizes": []},
                    "icon_margin_left": {"unit": "%", "top": "1", "right": "0", "bottom": "0", "left": "0", "isLinked": false},
                    "selected_icon": {"value": "fas fa-chess", "library": "fa-solid"},
                    "__fa4_migrated": {"selected_icon": true},
                    "text": "<p>We help businesses develop comprehensive strategic plans for growth and success.</p>",
                    "text_typography_font_size": {"unit": "px", "size": 15, "sizes": []},
                    "text_typography_font_style": "italic",
                    "text_margin": {"unit": "px", "top": "15", "right": "0", "bottom": "-30", "left": "0", "isLinked": false},
                    "icon_color": "#ffffff",
                    "iconbg_color": "#b68c2f",
                    "icon_bordering_border": "solid",
                    "icon_bordering_color": "#fafafa",
                    "_padding": {"unit": "px", "top": "30", "right": "30", "bottom": "30", "left": "30", "isLinked": true},
                    "_border_width": {"unit": "px", "top": "0", "right": "1", "bottom": "1", "left": "1", "isLinked": false},
                    "_border_color": "#b68c2f",
                    "_border_border": "dashed",
                    "icon_margin": {"unit": "px", "top": "-27", "right": 0, "bottom": "0", "left": 0, "isLinked": false},
                    "icon_bordering_width": {"unit": "px", "top": "7", "right": "7", "bottom": "7", "left": "7", "isLinked": true},
                    "subtitle": "Professional Consulting",
                    "btn_margin": {"unit": "%", "top": "", "right": "", "bottom": "", "left": "", "isLinked": false},
                    "icon_lheight": {"unit": "px", "size": 58, "sizes": []}
                },
                "elements": [],
                "isInner": false,
                "widgetType": "cholot-texticon",
                "elType": "widget"
            }''',
            
            # Response 2: Gemini's Web Development response (has minor issues)
            '''{
                "id": "a1b2c3d4",
                "settings": {
                    "icon": "fa fa-child",
                    "title_text_margin": {"unit": "px", "size": 50, "sizes": []},
                    "title": "Web Development",
                    "title_typography_typography": "custom",
                    "title_typography_font_size": {"unit": "px", "size": 28, "sizes": []},
                    "title_margin": {"unit": "px", "top": "0", "right": "0", "bottom": "15", "left": "0", "isLinked": false},
                    "subtitle_typography_typography": "custom",
                    "subtitle_typography_font_size": {"unit": "px", "size": 13, "sizes": []},
                    "subtitle_typography_font_weight": "700",
                    "subtitle_typography_text_transform": "uppercase",
                    "subtitle_typography_letter_spacing": {"unit": "px", "size": 1, "sizes": []},
                    "sb_padding": {"unit": "%", "top": "", "right": "", "bottom": "", "left": "", "isLinked": false},
                    "sb_margin": {"unit": "px", "top": "0", "right": "0", "bottom": "-15", "left": "0", "isLinked": false},
                    "subtitle_color": "#b68c2f",
                    "icon_size": {"unit": "px", "size": 20, "sizes": []},
                    "icon_bg_size": {"unit": "px", "size": 72, "sizes": []},
                    "icon_margin_left": {"unit": "%", "top": "1", "right": "0", "bottom": "0", "left": "0", "isLinked": false},
                    "selected_icon": {"value": "fas fa-code", "library": "fa-solid"},
                    "__fa4_migrated": {"selected_icon": true},
                    "text": "<p>Custom web applications built with modern technologies and best practices.</p>",
                    "text_typography_font_size": {"unit": "px", "size": 15, "sizes": []},
                    "text_typography_font_style": "italic",
                    "text_margin": {"unit": "px", "top": "15", "right": "0", "bottom": "-30", "left": "0", "isLinked": false},
                    "icon_color": "#ffffff",
                    "iconbg_color": "#b68c2f",
                    "icon_bordering_border": "solid",
                    "icon_bordering_color": "#fafafa",
                    "_padding": {"unit": "px", "top": "30", "right": "30", "bottom": "30", "left": "30", "isLinked": true},
                    "_border_width": {"unit": "px", "top": "0", "right": "1", "bottom": "1", "left": "1", "isLinked": false},
                    "_border_color": "#b68c2f",
                    "_border_border": "dashed",
                    "icon_margin": {"unit": "px", "top": "-27", "right": 0, "bottom": "0", "left": 0, "isLinked": false},
                    "icon_bordering_width": {"unit": "px", "top": "7", "right": "7", "bottom": "7", "left": "7", "isLinked": true},
                    "subtitle": "TechFlow Solutions",
                    "btn_margin": {"unit": "%", "top": "", "right": "", "bottom": "", "left": "", "isLinked": false},
                    "icon_lheight": {"unit": "px", "size": 58, "sizes": []}
                },
                "elements": [],
                "isInner": false,
                "widgetType": "cholot-texticon",
                "elType": "widget"
            }''',
            
            # Response 3: Simulated poor structure for comparison
            '''[{
                "id": "xyz789",
                "settings": {"gap": "extended"},
                "elements": [{
                    "title": "Marketing Services",
                    "icon": "fas fa-bullhorn"
                }]
            }]'''
        ]
        
        # Use different responses for different tests
        response_index = len(self.test_results) % len(actual_responses)
        llm_response = actual_responses[response_index]
        
        # Validate the response
        validation_result = self.validator.validate_structure(llm_response)
        
        result = {
            'test_name': test_name,
            'business_prompt': business_prompt,
            'llm_response': llm_response,
            'validation': validation_result,
            'timestamp': time.time()
        }
        
        self.test_results.append(result)
        
        # Print immediate results
        status = "‚úÖ PASSED" if validation_result['valid'] else "‚ùå FAILED"
        print(f"   {status} - Score: {validation_result['score']:.1f}/100")
        
        if validation_result['errors']:
            print("   Errors:")
            for error in validation_result['errors'][:3]:  # Show first 3
                print(f"     ‚Ä¢ {error}")
        
        if validation_result['warnings']:
            print("   Warnings:")
            for warning in validation_result['warnings'][:2]:  # Show first 2
                print(f"     ‚Ä¢ {warning}")
        
        return result
    
    def generate_report(self) -> Dict:
        """Generate comprehensive test report."""
        if not self.test_results:
            return {"error": "No test results available"}
        
        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results if r['validation']['valid'])
        average_score = sum(r['validation']['score'] for r in self.test_results) / total_tests
        
        # Analyze common errors
        all_errors = []
        all_warnings = []
        for result in self.test_results:
            all_errors.extend(result['validation']['errors'])
            all_warnings.extend(result['validation']['warnings'])
        
        # Count error patterns
        error_patterns = {}
        for error in all_errors:
            pattern = error.split(':')[0] if ':' in error else error
            error_patterns[pattern] = error_patterns.get(pattern, 0) + 1
        
        report = {
            'summary': {
                'total_tests': total_tests,
                'passed_tests': passed_tests,
                'failed_tests': total_tests - passed_tests,
                'success_rate': (passed_tests / total_tests) * 100,
                'average_score': average_score
            },
            'error_analysis': {
                'common_errors': dict(sorted(error_patterns.items(), key=lambda x: x[1], reverse=True)[:5]),
                'total_errors': len(all_errors),
                'total_warnings': len(all_warnings)
            },
            'detailed_results': self.test_results
        }
        
        return report


def main():
    """Main test execution function."""
    print("üöÄ Starting LLM Elementor JSON Generation Tests")
    print("=" * 60)
    
    runner = LLMTestRunner()
    
    # Extract example widget from real template
    template_path = "/Users/holgerbrandt/dev/claude-code/projects/riman-wordpress/riman-wordpress-cholot/wordpress/templates/home-page.xml"
    example_widget = runner.extract_example_widget(template_path)
    
    if example_widget:
        print(f"‚úÖ Extracted example widget: {example_widget['widgetType']}")
        print(f"   Widget has {len(example_widget['settings'])} settings")
    else:
        print("‚ùå Could not extract example widget")
        return
    
    # Define test scenarios
    test_scenarios = [
        {
            'name': 'Simple Business Services',
            'prompt': '''Create a cholot-texticon widget for "Professional Consulting" service with briefcase icon and description about expert advice.'''
        },
        {
            'name': 'Tech Company Services', 
            'prompt': '''Generate cholot-texticon widget for "Web Development" with code icon and text about creating modern websites.'''
        },
        {
            'name': 'Marketing Services',
            'prompt': '''Make a cholot-texticon widget for "Digital Marketing" with bullhorn icon and description of marketing expertise.'''
        }
    ]
    
    # Run tests
    for scenario in test_scenarios:
        runner.run_test(scenario['prompt'], example_widget, scenario['name'])
    
    # Generate and save report
    report = runner.generate_report()
    
    print("\n" + "=" * 60)
    print("üìä TEST RESULTS SUMMARY")
    print("=" * 60)
    
    summary = report['summary']
    print(f"Total Tests: {summary['total_tests']}")
    print(f"Passed: {summary['passed_tests']} ({summary['success_rate']:.1f}%)")
    print(f"Failed: {summary['failed_tests']}")
    print(f"Average Score: {summary['average_score']:.1f}/100")
    
    print(f"\nüîç COMMON ERRORS:")
    for error, count in report['error_analysis']['common_errors'].items():
        print(f"   ‚Ä¢ {error}: {count} times")
    
    # Save detailed report
    report_file = "llm-generation-test-results.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2)
    
    print(f"\nüìã Detailed report saved to: {report_file}")
    
    # Conclusions
    print(f"\nüéØ CONCLUSIONS:")
    if summary['success_rate'] >= 80:
        print("   ‚úÖ LLM shows HIGH capability for Elementor JSON generation")
    elif summary['success_rate'] >= 60:
        print("   ‚ö†Ô∏è  LLM shows MODERATE capability - needs guidance")
    else:
        print("   ‚ùå LLM shows LOW capability - requires significant templates")
        
    print(f"   Average quality score: {summary['average_score']:.1f}/100")


if __name__ == "__main__":
    main()